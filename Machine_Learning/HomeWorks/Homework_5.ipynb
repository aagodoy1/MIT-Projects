{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aebcbb44",
   "metadata": {},
   "source": [
    "In class Excersices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea86693f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergió en 5 iteraciones\n",
      "V_0^* = [0. 0. 0. 0. 0.]\n",
      "V_1^* = [0. 0. 0. 0. 1.]\n",
      "V_2^* = [0.  0.  0.  0.5 1. ]\n",
      "V_3^* = [0.   0.   0.25 0.5  1.  ]\n",
      "V_4^* = [0.    0.125 0.25  0.5   1.   ]\n",
      "V_5^* = [0.0625 0.125  0.25   0.5    1.    ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "gamma = 0.5\n",
    "n_states = 5\n",
    "stop_value = 1e-6\n",
    "\n",
    "V = np.zeros(n_states)\n",
    "V_history = [V.copy()]\n",
    "\n",
    "def reward(s):\n",
    "    return 1 if s == n_states - 1 else 0\n",
    "\n",
    "converged = False\n",
    "iteration = 0\n",
    "\n",
    "while True:\n",
    "    V_new = V.copy()\n",
    "    for s in range(n_states - 1):  # Último estado no cambia\n",
    "        V_new[s] = reward(s) + gamma * V[s + 1]\n",
    "    V_new[-1] = 1  # Estado terminal mantiene su valor\n",
    "\n",
    "    delta = np.max(np.abs(V_new - V))\n",
    "    V = V_new\n",
    "    if delta < stop_value:\n",
    "        break\n",
    "    V_history.append(V.copy())\n",
    "    iteration += 1\n",
    "\n",
    "# Mostrar resultados\n",
    "print(f\"Convergió en {iteration} iteraciones\")\n",
    "for i, v in enumerate(V_history):\n",
    "    print(f\"V_{i}^* = {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4523e3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valor de cada estado tras 100 iteraciones:\n",
      "V(0) = 0.0076\n",
      "V(1) = 0.0242\n",
      "V(2) = 0.0990\n",
      "V(3) = 0.3990\n",
      "V(4) = 1.5990\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Parámetros del entorno\n",
    "n_states = 5\n",
    "gamma = 0.5\n",
    "n_iterations = 10\n",
    "actions = ['stay', 'left', 'right']\n",
    "\n",
    "# Recompensa: solo al llegar al último estado\n",
    "def reward(s):\n",
    "    return 1 if s == n_states - 1 else 0\n",
    "\n",
    "# Transición estocástica: devuelve lista de (siguiente_estado, probabilidad)\n",
    "def transition(s, action):\n",
    "    transitions = []\n",
    "\n",
    "    if action == 'stay':\n",
    "        if s == 0 or s == n_states - 1:\n",
    "            # En bordes: 50% quedarse, 50% moverse al único vecino\n",
    "            transitions.append((s, 0.5))\n",
    "            neighbor = s + 1 if s == 0 else s - 1\n",
    "            transitions.append((neighbor, 0.5))\n",
    "        else:\n",
    "            # En interior: 25% izquierda, 50% quedarse, 25% derecha\n",
    "            transitions.append((s - 1, 0.25))\n",
    "            transitions.append((s, 0.5))\n",
    "            transitions.append((s + 1, 0.25))\n",
    "\n",
    "    elif action == 'left':\n",
    "        if s == 0:\n",
    "            # Igual que stay en el borde izquierdo\n",
    "            transitions.append((s, 0.5))\n",
    "            transitions.append((s + 1, 0.5))\n",
    "        else:\n",
    "            # 1/3 chance de moverse, 2/3 de fallar (y quedarse)\n",
    "            target = s - 1\n",
    "            transitions.append((s, 2 / 3))     # Falla\n",
    "            transitions.append((target, 1 / 3))  # Éxito\n",
    "\n",
    "    elif action == 'right':\n",
    "        if s == n_states - 1:\n",
    "            # Igual que stay en el borde derecho\n",
    "            transitions.append((s, 0.5))\n",
    "            transitions.append((s - 1, 0.5))\n",
    "        else:\n",
    "            target = s + 1\n",
    "            transitions.append((s, 2 / 3))     # Falla\n",
    "            transitions.append((target, 1 / 3))  # Éxito\n",
    "\n",
    "    return transitions\n",
    "\n",
    "# Inicializar valores\n",
    "V = np.zeros(n_states)\n",
    "\n",
    "# Value Iteration\n",
    "for it in range(n_iterations):\n",
    "    V_new = np.zeros(n_states)\n",
    "    for s in range(n_states):\n",
    "        values = []\n",
    "        for action in actions:\n",
    "            expected_value = 0\n",
    "            for s_next, prob in transition(s, action):\n",
    "                expected_value += prob * (reward(s) + gamma * V[s_next])\n",
    "            values.append(expected_value)\n",
    "        V_new[s] = max(values)\n",
    "    V = V_new\n",
    "\n",
    "# Resultados\n",
    "print(\"Valor de cada estado tras 100 iteraciones:\")\n",
    "for i, v in enumerate(V):\n",
    "    print(f\"V({i}) = {v:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdee5da",
   "metadata": {},
   "source": [
    "# Start of HomeWork 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7642876e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esperado de CVW es $1369, y esperado de albion es $2020\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "albion_revenew = np.array([10**2, 70**2])\n",
    "albion_probs = np.array([0.6, 0.4])\n",
    "albion_expected = albion_revenew@albion_probs\n",
    "\n",
    "CVW_expected = 37\n",
    "\n",
    "print(f'Esperado de CVW es ${CVW_expected**2}, y esperado de albion es ${int(albion_expected)}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb6306d",
   "metadata": {},
   "source": [
    "# Problem 2: 2. Q-Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc9414a",
   "metadata": {},
   "source": [
    "### Calculate Q1(s,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150f6403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q(0, M): 0\n",
      "Q(0, C): 0\n",
      "Q(1, M): 1.0\n",
      "Q(1, C): 1.0161088135763985\n",
      "Q(2, M): 1.0\n",
      "Q(2, C): 1.00441922206557\n",
      "Q(3, M): 1.0\n",
      "Q(3, C): 0.9953340768291793\n",
      "Q(4, M): 1.0\n",
      "Q(4, C): 0.3535533905932738\n",
      "Q(5, M): 1.0\n",
      "Q(5, C): 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "actions = ['M', 'C']\n",
    "states = [0, 1, 2, 3, 4, 5]\n",
    "gamma = 0.6\n",
    "def reward(s1, s2):\n",
    "    # s: states -> [0, 1, 2, 3, 4, 5]\n",
    "    # Retorn valor del reward\n",
    "    if s1 == s2 and s1 == 0:\n",
    "        return 0\n",
    "    elif s1 != s2:\n",
    "        return abs(s1-s2)**(1/3)\n",
    "    elif s1 == s2:\n",
    "        return (s1+4)**(-1/2)\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def transitions(s, a):\n",
    "    # s: states -> [0, 1, 2, 3, 4, 5]\n",
    "    # a: action ->  ['M', 'C']\n",
    "\n",
    "    # Retorna estado_final y prob de este\n",
    "    if s == 0:\n",
    "        return [[0, 1]]\n",
    "    elif s in (1,2,3):\n",
    "        if a == 'M':\n",
    "            return [[s-1, 1]]\n",
    "        elif a == 'C':\n",
    "            return [[s+2, 0.7], [s, 0.3]]\n",
    "    elif s in (4,5):\n",
    "         if a == 'M':\n",
    "            return [[s-1, 1]]\n",
    "         elif a == 'C':\n",
    "            return [[s, 1]]\n",
    "\n",
    "#Calcular los Q para Q(s,a)\n",
    "\n",
    "for s in states:\n",
    "    for a in actions:\n",
    "        Q = 0\n",
    "        trans = transitions(s,a)\n",
    "        for tran in trans:\n",
    "            # prob * reward de prob\n",
    "            Q += tran[1] * reward(s,tran[0])\n",
    "        print(f'Q({s}, {a}): {Q}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ca9768",
   "metadata": {},
   "source": [
    "### Calculate de V1(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "60c59c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V1(0): 0\n",
      "V1(1): 1.0161088135763985\n",
      "V1(2): 1.00441922206557\n",
      "V1(3): 1.0\n",
      "V1(4): 1.0\n",
      "V1(5): 1.0\n"
     ]
    }
   ],
   "source": [
    "for s in states:\n",
    "    best_value = []\n",
    "\n",
    "    for a in actions:\n",
    "        Q = 0\n",
    "        trans = transitions(s,a)\n",
    "        for tran in trans:\n",
    "            # prob * reward de prob\n",
    "            Q += tran[1] * reward(s,tran[0])\n",
    "        best_value.append(Q)\n",
    "    print(f'V1({s}): {max(best_value)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e419474",
   "metadata": {},
   "source": [
    "Encuentra la mejor policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c55a5da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micromasters_ml_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
